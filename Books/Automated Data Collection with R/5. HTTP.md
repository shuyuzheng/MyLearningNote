**H**yper**t**ext **T**ransfer **P**rotocol (HTTP): is the _lingua franca_ of communication on the Web.

# 1. HTTP fundamentals
## 1.1 a short conversation with a web server
* **Internet Protocol Suite (IPS)**: the techniques, standards and protocols that serves behind the program layer for the daily web accessing.
* _transportation_ standards:
    * TCP (Transmission Control Protocol): represents the transportation layer.
    * IP (Internet Protocol): represents the Internet layer.
* message exchange protocols: They define standard vocabulary and procedures for clients and servers to _talk_ about specific tasks (retrieving or storing documents, files, messaged, and so forth). They are subsumed under the label **application layer**
    * website message:
        * HTTP (Hyper Text Transfer Protocol)
        * FTP (File Transfer protocol)
    * Email retrieval:
        * POP (Post Office Protocol)
    * Email storage and retrieval:
        * SMTP (Simple Mail Transfer Protocol)
        * IMAP (Internet Message Access Protocol)

* DNS (Domain Name System): receive the URL from _client_ and return a corresponding _IP address_.
* **IP address** was used to establish a connection between _client_ and HTTP _server_ for further information exchanging.
* Important things for HTTP:
    1. HTTP is **not only** a protocol to transport hypertext documents but is used for **all kinds of** resources.
    2. HTTP is a stateless protocol.

* **HTTP port**: It is like a _door_ at the server's _house_ where the HTTP client knocks. The default port for HTTP is 80.

## 1.2 URL syntax
URL (Uniform Resource Locators): `scheme://hostname:port/path?querystring#fragment`

* Each URL starts with a **scheme** that defines the _protocol_ that is used to communicate between client/application and server. Including:
    * HTTP
    * ftp or mailto: corresponds to email addresses that rely on SMTP standard.

* **hostname**: the name of the _server_ where the resource of interest is stored.
* **port** tells the client at which door it has to knock in order to get access to the requested resource. If you are fine with the default port, this part **can be dropped**.
* **IP address**: The *machine-readable* version of _hostname_. It can be generated by DNS from URL.
* **Path**: determines the **location** of the requested resource on the server.
* **query strings**: contains one or more **name=value** pairs. It start with `?` and use `&` to separate multiple name-value pairs.
* **Fragments** help point to a **specific part** of a document. Fragments are handled by the browser, that is, on the client side. (After the server has returned the whole document, the fragment is used to display the specified part.)

* **URL encoding**: URL are transmitted using the ASII character set, any characters not included in this set should be escaped by using **URL encoding**. It use `%` to initializes each of the encodings. Details can be found in [ref_urlencode](https://www.w3schools.com/tags/ref_urlencode.asp)

* R functions for encoding and decoding URL: `URLencod()`, `URLdecode()`. The _reserve_ argument in the encoding function ensures that non-alphanumeric characters are encoded with their percent-encoding representation.

## 1.3 HTTP messages
* The structure of client requests or server response messages:
    * **start line**: The first line of HTTP message. 
        * In **HTTP request message**, it defines:
            * the [method] used for the request,
            * [path] to the resource,
            * highest HTTP [version] the client can handle.
        * In **HTTP response message**, it response:
            * the highest HTTP [version] the servers can handle.
            * A [status] code, 
            * a human-readable **explanation** of the status.
    * **header**: provide **meta information** about the other sides' preferences or the content sent along with the message. 
        * each header field is places on a _new line_
        * header field name and header field value are separated _colon_: `[header name:] [header value]` 
        * If header line is very long, it can be divided into several lines by  beginning the additional line with an _empty space_ character.
    * **body**: contains the data.
        * It might be _plain text_ or _binary data_.
        * The type of data is specified by _content-type_ header, following the MIME type specification (Multipurpose Internet Mail Extensions)
        * MIME tell the client or server which type of data it should expect. `main-type/sub-type`. THe list of MIME can be found in [IANA](http://www.iana.org/assignments/media-types/media-types.xhtml)
    * Three parts are separated by **carriage return and line feed characters (CRLF)**: _start line_ [CRLF] _header_ [CRLF] [CRLF] _body_. In R, these characters are represented as escaped characters `\r` for carriage return and `\n` for new line feed.

## 1.4 Request method

Method  |                                          Description
------- | ---------------------------------------------------------------------------------------------
GET     | **Retrieves** resource from server
POST    | **Retrieves** resource from server using the message body to send data or files to the server
HEAD    | Works like GET, but server responds only the _start line_ and _header_, **no body**
PUT     | **Stores** the body of the request message on the server.
DELETE  | **Deletes** a resource from the server
TRACE   | **Traces the route** of the message along its way to the server.
OPTIONS | Returns list of supported HTTP methods
CONNECT | Establishes a network connection

## 1.5 Status codes 
* Status codes can range from **100 to 599**:
    * 1xx, for information
    * 2xx, for success
    * 3xx, for redirection
    * 4xx, for client errors
    * 5xx, for server errors

## 1.6 Header fields
* `Accept` (request): tells the server about the **type of resources** the client is willing to accept as response.
* **content-type** preference:
    1. more specific types are preferred over less specific ones
    2. types are preferred in decreasing order of the `q` parameter
    3. all type specifications have a default preference of `q = 1` if not specified otherwise.
* `Accept-Encoding` (request): tells the server which **encodings** or **compression methods** are sccepted by the client.
* `Allow` (response;body): informs the client about the HTTP methods that are allowed for particular resource and will be part of responses with a status code of 405.
* `Authorization` (request): is a simple way of sending **passing username and password** to the server. Username and password $\rightarrow$ `username:password` $\rightarrow$ encoded according to the _Base64_ scheme.
* `Content-Encoding` (response;body): specifies the transformations (e.g. compression methods) that have been applied to the content.
* `Content-Length` (response;body): provide the receiver of the message with information on the **size** of then content in decimal number of OCTETs(bytes).
* `Content-Type`(response;body): provides information on the **type** of content in the body. (MIME types)
* `Cookie`(request): it returns the previously received information -- without cookies servers would not know that they have had contact with a client before. It consist of `name-value` pairs that separated by semicolon `;`.
    * expires: a date after which cookie is no longer valid.
    * domain and path: specify for which resource request the cookie is needed.
    * secure: indicate that the cookie should **only** be sent over **secured connections** (SSL)
* `Form` (response): provides programmers of web crawlers or scraping programs with the option to send their email address. This helps webmasters to contact those who are in control of automated robots and web crawlers if they observe unauthorized behavior.
* `Host`(request): requests and helps servers to decide upon ambiguous URLs when more than one host name redirected to the same IP address.
* `If-Modified-Since`(request): is used to make requests conditional on the time stamp associated with the request resource.
* `Connection`(request, response): 
    * In HTTP/1.1, connections are **persistent** by default.(client and server keep their connection alive after the request-response procedure)
    * In HTTP/1.0, connections are **close** by default.
    * Specify: `Connection: Keep-Alive`; `Connection: Close`
* `Last-Modified`(response;body): provides the date and time stamp of the last modification of the resource.
* `Location`(response;body): redirect the receiver of a message to the location where the requested resource can be found.
* `Proxy-Authorization`(request): The same as `Authorization`, only for proxy servers.
* `Proxy-Connection`(request): The same as `Connection`, only for proxy server.
* `Referer`(request): informs the server **what referred** to be requested resource.
* `server` (response): provides information about the server addressed in the request.
* `Set-Cookie`(response): asks the client to store the information contained in the `set-cookie` header field and send then along in subsequent requests as part of the `cookie` header.
* `User-Agent`(request): indicates the type of client that makes a request to the server.
* `Vary`(response): The server response sometimes depends on certain parameters (like browser or device of the client, whether the user has previously visited a site and have received a cookie, and the encoding format the client accept). Servers can indicate that content changes according to these parameters with the `Vary` header field.
* `Via`(request, response): is like `Server` but for proxy servers and gateways that HTTP messages pass on their way to the server or client.
* `WWW-Authenticate`(response): asks the client to identify itself and is sent along a `401 Unauthorized` status code.

# 2. Advanced features of HTTP
## 2.1 Identification
### 2.1.1 HTTP header fields for client Identification

1. **User-Agent** header field: It contains the information about the software that is used on the client side.
    * We can type some information about our scrapping (R software version, platform of R) to keep our work as **transparent** as possible.
    * It can be delivered when you send the `Get` request (**RCurl** package):
    
        ```
         cat(getURL("http://httpbin.org/headers",
                    useragent = str_c(R.version$platform,
                                        R.version$version.string,
                                        sep=", ")))
        {
        "headers": {
            "X-Request-Id": "0726a0cf-a26a-43b9-b5a4-9578d0be712b",
            "User-Agent": "x86_64-w64-mingw32, R version 3.0.2 (2013-09-25)",
            "Connection": "close",
            "Accept": "*/*",
            "Host": "httpbin.org"
            }
        }
        ```
    * `cat()`: It is used to concatenate and print the results over several lines.

2. **Referer** header field:
    * Contains: the URL of the page that referred the user to the current page.
    * Usage:
        * traffic evaluation: to asses where visitors of a site come from.
        * allows web designer to limit access to **specific server content**
    * Create:

        ```
        getURL("getURL("http://httpbin.org/headers", referer = "http://www.rdatacollection.com/")
        ```
    * **Referer proofing**: Provide the wrong referer to server to disguise the source of the access request.

3. **From** header field:
    * Contains: the user's email address.
    * Usage: Providing contact details signals good intentions and enables webmasters who note unusual traffic patterns on their sites to get in touch.
    * Create:

        ```
        getURL("http://httpbin.org/headers", httpheader = c(From = "eddie@r-collection.com"))
        ```
### 2.1.2 Cookies
1. How does it work?
    1. Web servers store a **unique session ID** in a cookie that is placed on the client's **local drive**, usually in a text file.
    2. The next time a browser sends an HTTP request to the **same server**, it looks for stored cookies that **belong to the server**.
    3. If it find that, it will add the cookie information to the request.
    4. The server will adapt their response according to this information and the **information stored previously about user** on the server.
2. What is in Cookie?
    * id: an unique id for server to identify the user in a subsequent request.
    * domain: which domain the cookie is associated with.
    * a set of other **name-value** pairs.
3. Create:
    1. Client makes a request to a web server.
    2. server responds and passes the cookie in response `Set-cookie` header field.
4. Types of cookie:
    * **session cookies**: are kept in memory **only as long as** the user visits a website and are **deleted** as soon as the browser is closed.
    * **Persistent cookies**(os tracking cookies): It's lifetime is defined by the value of the **max-age** or the **expires**.
    * **Three-party cookies**: personalize content across different sites. They do not belong to the domain the client visits but to another domain.

## 2.2 Authentication
* A set of authentication techniques exist that allow qualified access to **confidential content**. Some of these techniques are part of the HTTP protocol. Others, like _OpenID_ or _OAuth_, have been developed to extend authentication functionality on the Web.
* **Basic authentication**: The simplest form.
    1. The client requests a **protected resource**.
    2. The server asks the client for a **user name** and **password**. In `WWW-Authentication` header field.
    3. The client provides the requested username and password in **Base64 encoding**. In `Authorization` header field.
    4. The server returns the requested resource.
    * `base64(string)`: encoding _string_ into Base64 characters.
    * `base64Decode(base64)`: Decoding Base64 characters into string.
    * Basic authentication should **only** be used in combination with **HTTPS**, because the _user name-password_ information si easy to be decoded.
* **Digest authentication**: passwords are **never** sent across the Web in order to verify a user, but only a **"digest"** of it.
    1. The server attaches **nonce** (a little random string) to its response in `WWW-Authenticate` header field.
    2. The browser transforms _username_, _password_, and the _nonce_ into a **hash code**(following one of several algorithms that are known to both server and browser) in the `Authorization` head filed.
    3. This **hash code** is then sent back, compared to the hash calculations of the server. If both _hash code_ and _nonce_ match the server grants access to the client.

## 2.3 Proxies
* **Web proxy servers** or simply proxies, are servers that act as **intermediaries** between clients and other servers.
* Usage:
    * legal usage:
        * **speed up** network use;
        * stay **anonymous** on the Web;
    * **illegal** usage:
        * get access to sites that **restrict access** to IPs from certain locations;
        * get access to sites that are **normally blocked** in the country from where the request is put;
        * **keep on querying resources** from a server that blocks requests from IPs we have used before.
* The types of proxies:(based on the degree of anonymity)
    * **Transparent proxies**: specify a `Via` header field in their request to the server, filling it with their IP and provide an `X-Forwarded-For` header field with the user's IP.
    * **Simple anonymous proxies**: replace both the `Via` and the `X-Forwarded-For` header field with **their IP**.
    * **Distorting proxies**: replace the `X-Forwarded-For` header field with a **random** IP address.
    * **High anonymity proxies (elite proxies)**: They **neither** provide the `Via` nor the `X-Forwarded-For` header field but only their IP. They just behave like normal clients.
* To send a request to a server via a proxy with R:

    ```
    getURL("URL", proxy = "IP address:port", followlocation = T)
    ```

# 3. Protocol beyond HTTP
* There are many protocols that can be used for data transfer, not only HTTP. A list of the protocols that supported by **RCurl** package can be printed by: `curlVersion()$protocols`
## 3.1 HTTP secure
* **Hypertext Transfer Protocol Secure (HTTP)** = HTTP + SSL/TLS (**Secure Sockets Layer/Transport Security Layer).
* HTTPS URLs have the scheme `https` and use the port _443_ by default.
* The purpose of HTTPS:
    1. It helps the client to ensure that the server it talks to is **trustworthy**(server authentication).
    2. It provides **encryption** of client-server communication.
* A very simplified scheme of the "SSL handshake" is the **negotiation** between client and server about the establishment of an HTTPS connection _before_ actually exchanging encrypted HTTP messages:
    1. The client establishes a TCP connection to the server via port 443 and sends information about the **SSL version** and **cipher** settings.
    2. The server sends back information about the **SSL** and **cipher** settings, and a **certificate** (to prove his identity). The certificate contains:
        * information about the authority that issued the certificate
        * for whom it was issued
        * its period of validity
        * the **signature** of a trusted certificate authority (CA)
    3. The client checks if it trusts the certificate. (whether it is in the list of trusted CAs signed in either browser or operating system). If it is not in trust list, user should decided whether access this Website.
    4. By using the **public key** of HTTPs server, the client generates a **session key** that only the server can read, and sends it to the server.
    5. The server **decrypts** the session key.
    6. Both client and server now possess a session key. They use this key establish a **SSL tunnel** to transfer data.
* What is protected: the content of communication (HTTP headers, cookies, message body)
* What is **not** protected: IP address.

## 3.2 FTP
* The **File Transfer Protocol(FTP)** was developed to transfer files from client to server(upload), from server to client (download), and to manage directories.
* Drawbacks:
    * it does not support the persistent, keep-alive connections. The connections between client and server has to be **reestablished** after each transfer.
    * FTP does **not** natively support proxies and pipelining.
* Advantages: It just transfers the pure binary or ASCII files, without _header files_.
* Ports for FTP:
    * "data port" (20): for data exchange.
    * "control port" (21): for command exchange.
* Two modes for FTP connection:
    * **Active mode**: the client connects with the server's command port and then requests a data transfer to another port. (As in this mode, the actual data connection is established by the server, the client's firewall don't know that the client's request and blocks the data deliver)
    * **Passive mode**: The client initiates both the command and the data connection.

# 4. HTTP in action 
* `Base R` functions: `download.file()`, `connections` have some drawback for Web scraping.
    * It is not very flexible. It cannot connect HTTPS.
    * It lacks basic identification facilities.

## 4.1 The libcurl library
* _libcurl_ is an external library programmed in C.
* What can it do:
   * specify HTTP headers
   * interpret URL encoding
   * process incoming streams of data from web server
   * establish SSL connections
   * connect with proxies
   * handle authentication
   * ...
* `RCurl` and `httr` packages are based on `libcurl` library. `names(ger CurlOptionsConstants())` could be used to see the comprehensive list of _libcurl's_ "easy" interface options that can be specified with RCurl.

## 4.2 Basic request methods
### 4.2.1 The GET method
* High-level functions provided by RCurl:
    * `getURL()`
    * `getBinaryURL()`: when expected content is binary.
    * `getURLContent()`: It tries to identify the type of content by inspecting the `content-type` field in the response header and proceeding adequately.
* How does the function work:
    1. After we passing the **URL**, the function **automatically** identifies the _host_, _port_, and _requested resource_.
    2. If the call succeeds (the server gives a 2XX response), the function returns the content of the response.
    3. `getURL()` returns the body as **character** data. `getBinaryURL()` returns the **raw content**
* `writeBin()`: write binary data to a connection or a raw vector.
* GET data from `form`:
    1. Generate a specific URL by str_c(_original URL_, "?", "attr_name1=", "attr_value1", "&", "attr_name2=", "attr_value2",...), and past it to `getURL()` function.
    2. `getForm()`: `getForm(url, attr_name1 = "attr_value1, attr_name2 = "attr_value2")`

### 4.2.2 The POST method
* The _POST_ method implies that parameters and their values are sent in the **request body**, not in the URL itself. 
* `postForm()`: `postForm(url, attr_name = "attr_value1", attr_name2 = "attr_value2", style = "post)`
* There are several ways to format the _name-value_ pairs in this function. Sometimes we have to explicitly specify the one accepted in advance using the _style_ argument. Details about form types can be found on [w3 form](https://www.w3.org/TR/html401/interact/forms.html).
* To find the adequate _POST_ format, we can look for an attribute named `enctype` in the <form> element:
    * `style = "post"`: for "application/x-www-form-urlencoded".
    * `style = "httppost"`: for the "multipart/form-data".

### 4.2.3 Other methods
* By set the `customrequest` option in `getURL()`, `getBinaryURL()` and `getURLContent()` functions, we can use other HTTP methods for make request.

## 4.3 A low-level function of RCurl
* `curlPerform()`: It gathers options specified in R on how to perform web requests (which protocol or methods to use, which headers to set) and patches them through to _libcurl_ to execute the request.
    * we have to explicitly set everything when using this function:
        1. We need to define a handler:
            * content handler:
                * `basicTextGatherer()`: turns an object into a list of functions that handles updates, resets and value retrieval.
            * debug handler: via `debugfunc`
            * HTTP header handler: via `headerfunc`
        2. Using `curlOptions()` to generate the perform options based on these handlers.
        3. Using `curlPerform()` with `opts = performOptions` to make the request.
        4. Using `value()` function of content we can extract the content that was set from the server.
    
    * example:

        ```
        content <- basicTextGatherer()
        header <- basicTextGatherer()
        debug <- debugGatherer()
        performOptions <- curlOptions(url = url,
                                    writefunc = content$update,
                                    headerfunc = header$update,
                                    debugfunc = debug$update,
                                    verbose = T)
        curlPerform(.opts=performOptions)
        header$value()
        debug$value()
        ```

## 4.4 Maintaining connections across multiple request
* Reusing connections works with the so-called **curl handles**. They serve as **containers** for the connection itself and additional features/options.
* Create: `getCurlHandle()`
* Use: It can be used for multiple requests using the `curl` argument. `lapply(urls, gerURL, curl = handle)`
* When should we use curl handles?
    * Specifying and using curl options across an entire session with RCurl, simplifying our code and making it more reliable.
    * fetching a bunch of resources from the same server is faster when we reuse the same connection.

## 4.5 options
* classes of RCurl options:
    * define the behavior of the underlying _libcurl_ library
    * define how information is handled in R.
* **Declare** options:
    * In the **single** call to the high-level functions. The options will **only** affect the **single** function call.
    * To bind them to a **curl handle**. Every functions using this handle will use the same options.
        * If a function uses the handle and redefines or adds some options these changes will **stick** to the handle.
    * With `dupCurlHandle()` we can **copy** the options set in one handle to another.
    * **Global options**:
        * define a list of options save it in a **object**(`curlOptions()` was used here), and pass it to `.opts` when initializing a handle or calling a function.
        * Using **R's global option system** to specify standard values that will be part of **each** curl handle or function call. `options(RCurlOptions = list(parameter-value pairs))`
* `customrequest` option: control HTTP methods. values: POST, HEAD, or PUT ...
* `httpheaders` option: control HTTP heads. values: a list where names of the list items identify the header name and their values correspond to header values. [a list of other conventional header fields](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html)
* Default options are recommend for each request: (could be setting directly using `option()` at the beginning and the end of the url request process).

```
defaultOptions <- curlOptions(
                    httpheader = list(
                    from = "Eddie@r-datacollection.com",
                    'user-agent' = str_c(R.version$platform,
                                        R.version$version.string,
                                        sep=", ")),
                    followlocation = TRUE,
                    maxredirs = 10,
                    connecttimeout = 10,
                    timeout = 300,
                    cookiefile = "RCurlCookies.txt",
                    cainfo = system.file("CurlSSL","cacert.pem", package = "RCurl"))
        
options(RCurlOptions = defaultOptions)
```
## 4.6 Debugging
1. generally useful tool for HTTP debugging is the service at http://httbin.org. It provides a set of endpoint for specific HTTP requests. To check whether a GET request is specified correctly and what information arrives at the server:

```
library(RCurl)
url <- "httpbin.org/get"
res <- getURL(url = url)
cat(res)
```

2. RCurl debugging functions: Specify a **debug gatherer** within the function call:
    1. create an object that contains three functions: `update()`, `value()`, `reset()` by calling the `debugGatherer()`:

        ```
        debugInfo <- debugGatherer()
        ```
    2. request a document using the `getURL()` function with the `debugfunction` option. Note: set the `verbose` option to `TRUE`

        ```
        url <- "r-datacollection.com/materials/http/helloworld.html"
        res <- getURL(url = url, debugfunction = debugInfo$update, verbose = T)
        ```
    3. **access** the debugging information by calling the `value()` function stored in the `debugInfo` object. (e.g. `names(debugInfo$value())`)

3. Apply `gerCurlInfo()` to the `handle` used in the function call.
    * `handleInfo["total.time"]`: the total time it took to complete the request
    * `handleInfo["pretransfer.time"]`: the time it took to do all the things necessary to start the transfer: resolve the host name, establish the connection to the host and send the request.

## 4.7 Error handling
* Calling `getCurlErrorClassNames()` function shows the list of error types, the most common ones:

    ```
    getCurlErrorClassNames()[c(2:4, 7, 8, 10, 23, 29, 35, 64)]
    [1] "UNSUPPORTED_PROTOCOL" "FAILED_INIT" "URL_MALFORMAT"
    [4] "COULDNT_RESOLVE_HOST" "COULDNT_CONNECT" "REMOTE_ACCESS_DENIED"
    [7] "HTTP_RETURNED_ERROR" "OPERATION_TIMEDOUT" "HTTP_POST_ERROR"
    [10] "FILESIZE_EXCEEDED"
    ```
* Using `tryCatch()` we can specify **individual actions** to react to different types of errors.

## 4.8 RCurl or httr?
* `RCurl` is more powerful and sophisticated
* `httr` is more light weight and can ease some data collection tasks, like authentication via _OAuth_.
* The difference between these two packages are listed in the t textbook "Automated data collection with R" p145-146