# Introduction
The circle of scraping the web: (the art of scraping lies in cleverly combining and redefining these tasks)

1. Information identification
2. choice of strategy
3. Data retrieval
4. Information extraction
5. Data preparation
6. Data validation
7. Debugging and maintenance
8. Generalization

* **scrapers** are programs that grab **specific** content from web pages.
* **Spiders (or crawlers or web robots)** are programs that grab and index **entire pages** and move around the Web following **every link** they can find.

# 1. Retrieval scenarios
* R packages: `RCurl`, `XML`, `stringr`

## 1.1 Downloading ready-made files
* Sometimes you can find data of interest ready for download in TXT, CSV or any other plain-text/spreadsheet or binary format like PDF, XLS, or JPEG.
* Why use R in this scenarios?
    * the data acquisition process remains **reproducible** in principle.
    * it may **save a considerable amount of time**.

### 1.1.1 CSV election results data
* Data source:  http://www.elections.state.md.us/
* Steps to get the table interested from a bunch of tables on the website.
   1. Identify the **links** to the desired files.
        * `getHTMLLinks()`: extract all the URLs and external file names in the HTML document.
        * Using **Regex** to retrieve the subset of external file names that point to the CSVs that you are interested.
        * Store the names of CSVs links into a **list** (`filename_list`)for further operation.

        ```
        url <- "http://www.elections.state.md.us/elections/2012/election_
data/index.html"
        links <- getHTMLLinks(url)
        filenames <- links[str_detect(links, "_General.csv")]
        filenames_list <- as.list(filenames)
        filenames_list[1:3]
        ```

   2. Construct a **download function**: set up a function to download all the files and call the function `downloadCSV()` (this is a function wraps around the base R function `download.file()`)
            
            ```
            downloadCSV <- function(filename, baseurl, folder) {
                dir.create(folder, showWarnings = FALSE)
                fileurl <- str_c(baseurl, filename)
                if (!file.exists(str_c(folder, "/", filename))) {
                        download.file(fileurl,
                                 destfile = str_c(folder, "/", filename))
                        Sys.sleep(1)
                }
            }
            ```
    3. Execute the download: apply the function (`downloadVSC`) to the list of CSV file names (`filename_list`)
        
        ```
        library(plyr)
        l_ply(filenames_list, downloadCSV,
            baseurl = "www.elections.state.md.us/elections/2012/election_data/",
            folder = "elec12_maryland")
        ```
    4. Check the downloaded files:
        
        ```
        length(list.files("./elec12_maryland"))
        list.files("./elec12_maryland")[1:3]
        ```

### 1.1.2 PDF legislative district maps
* `download.file()` does **not** support data retrieval **via HTTPS** by default and is **not** capable of dealing with **cookies** or many other advanced features of HTTP.
* similar strategy as before but use the function from `RCurl` package to retrieve data:

```
downloadPDF <- function(filename, baseurl, folder, handle) {
    dir.create(folder, showWarnings = FALSE)
    fileurl <- str_c(baseurl, filename)
    if (!file.exists(str_c(folder, "/", filename))) {
            content <- getBinaryURL(fileurl, curl = handle)
            writeBin(content, str_c(folder, "/", filename))
            Sys.sleep(1)
    }
}
```
## 1.2 Downloading multiple files from an FTP index.
* load the FTP directory list into R: `getURL(url, dirlistonly = T)`
* It is sometimes the case that the default FTP mode in _libcrl_, extended passive(EPSV), does not work with some FTP servers. We have to add the **ftp.use.epsv = FALSE** option.
* To get 'HTML' file only: 

    ```
    filenames_html <- getURL(ftp, customrequest = "NLST *.html")
    filenames_html = str_split(filenames_html, "\\\r\\\n")
    ```
## 1.3 Manipulating URLs to access multiple pages
1. Identify the **running mechanism** in the URL syntax.
2. Retrieve links to the running pages.
3. download the running pages.
4. retrieve links to the entries on the running pages.
5. download the single entries.
* a function to retrieve the content of each pages:

```
dlPages <- function(pageurl, folder ,handle) {
    dir.create(folder, showWarnings = FALSE)
    page_name <- str_c(str_extract(pageurl, "/P.+"), ".html")
    if (page_name == "NA.html") { page_name <- "/base.html" }
    if (!file.exists(str_c(folder, "/", page_name))) {
            content <- try(getURL(pageurl, curl = handle))
            write(content, str_c(folder, "/", page_name))
            Sys.sleep(1)
    }
}
```
* `url.exists()` test whether a given URL exists.
* `file.exists()` test whether a file exists.
* PROS: Url manipulation allows us to access **all** the sites that we are interested in.
* CONS: It ask for a fairly intimate knowledge of the website and of the websites' directories.

## 1.4 convenient functions to gather links, lists, and tables from HTML documents
* `getHTMLLinks()`: extract links from HTML documents.
* `getHTMLExternalFiles()`: extract only **links** that point to external files.
* `getHTMLList()`: extract _list_ elements
* `getHTMLTable()`: extract _table_ elements and transform them into **dataframe**.
* `elFun` argument for `getHTMLList()` or `getHTMLTable()`: define individual element **functions** to each items of lit `<ls>` or each cell of table `<tb>`. (e.g. `readHTMLTable(url, elFun = getHTMLLinks))

## 1.5 Dealing with HTML forms
* The process steps:
    1. recognize the forms that are involved.
    2. determine the method used to transfer the data.
    3. determine the address to send the data to.
    4. determine the inputs to be sent along.
    5. build a valid request and send it out.
    6. process the returned resources.

* a function extract attributes of node into a dataframe
```
xmlAttrsToDF <- function(parsedHTML, xpath) {
    x <- xpathApply(parsedHTML, xpath, xmlAttrs)
    x <- lapply(x, function(x) as.data.frame(t(x)))
    do.call(rbind.fill, x)
}
```
### 1.5.1 GETing to grips with forms
1. using `xmlAttrsToDF()` checks the attributes of _Forms_.
2. using `xmlAttrsToFD()` checks the attributes of _input_.
3. make the request by `getForm()` with argument `s = "input"`
4. Another method: appending query string to the base URL and call `getURL()`