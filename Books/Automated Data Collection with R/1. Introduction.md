# 1. Deciding which data to collect:
1. Make sure you know exactly what kind of information you need.
2. Find out whether there are any data sources on the Web that might provide **direct or indirect information** on your problem.
3. Develope a theory of the data generation process when looking into potential sources.
    * When were the data generated?
    * When were they uploaded to the Web, and by whom?
    * Are there any potential areas that are not covered, consistent or accurate?
    * Are you able to identify and correct them?
4. Balance advantages and disadvantages of potential data sources.
    * **Availability** and **legality**.
    * **costs** of collection.
    * **compatibility** of new sources with existing research.
    * **acceptance** of the data source by others.
    * possible ways to validate the quality of your data.
        * Are there other, independent sources that provide similar information so that **random cross-checks** are possible?
    * In case of **secondary data**, can you identify the original source and check for **transfer errors**?
5. Make a decision.

# What makes data retrieving more difficult?
1. Data are stored in more complex structures than HTML tables.
2. Web pages are **dynamic**.
3. Information has to be retrieved from **plain text**.

# Three important field for data collection
1. Technologies for disseminating content on the Web
    * HTTP(Hypertext Transfer Protocol): A commonly used language of communication on the Web. 
        * XML/HTML: HTML is used to shape the **display** of information; The XML is used to store data.
        * JSON(JavaScript Object Notation): Another **standard data storage and exchange** format. It is compatible with many programming languages and software.
        * AJAX: It plays an important role in enabling websites to request data asynchronously in the background ot the browser session and update its visual appearance in a **dynamic fashion**
        * Plain text: It is part of every HTML, XML and JSON document, but it is **unstructured data**.
2. Technologies for information extraction
    * XPath: It is used to select specific pieces of information for **marked up** documents such as HTML, XML or any variant of it, for example SVG and RSS.
    * JSON parsers: A tool used to parse JSON file.
    * Selenium: It is used for extracting information from AJAX-enriched webpages. It allows us to work directly in the browser to avoid some problems caused by AJAX.
    * Regular expression: Used on plain text data.

3. Technologies for data storage
    * R: SQL; Binary formats; Plain-text formats

**Solutions** are available on the bookâ€™s website www.r-datacollection.com